{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import whois\n",
    "import datetime\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import tldextract\n",
    "import dns.resolver\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import ipaddress\n",
    "\n",
    "import ssl\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import socket\n",
    "import validators\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-13.0.1-arm64-arm-64bit\n",
      "Tensor Flow Version: 2.12.0\n",
      "Keras Version: 2.12.0\n",
      "\n",
      "Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:12:31) [Clang 14.0.6 ]\n",
      "Pandas 2.0.0\n",
      "Scikit-Learn 1.2.2\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# What version of Python do you have?\n",
    "import sys\n",
    "\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "import platform\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU') ) > 0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age(domain):\n",
    "    # Get the creation date of the domain\n",
    "    try:\n",
    "        creation_date = whois.whois(domain).creation_date\n",
    "\n",
    "        # Calculate the age of the website in years\n",
    "        if isinstance(creation_date, list):\n",
    "                creation_date = creation_date[0]\n",
    "        age = (datetime.datetime.now() - creation_date).days / 365\n",
    "    except:\n",
    "        age = 0\n",
    "    return age\n",
    "\n",
    "def get_domain(url):\n",
    "    sub, domain, suffix = tldextract.extract(url)\n",
    "    subdomain = f\"{domain}.{suffix}\" if sub == \"\" else f\"{sub}.{domain}.{suffix}\"\n",
    "    domain = f\"{domain}.{suffix}\"\n",
    "    return subdomain, domain\n",
    "\n",
    "def validate_url(url):\n",
    "    valid=validators.url(url)\n",
    "    return valid == True\n",
    "\n",
    "def get_request(url):\n",
    "     validation = validate_url(url)\n",
    "     if not validation:\n",
    "          url = \"http://\" + url\n",
    "     try:\n",
    "          response = requests.get(url, timeout=20)\n",
    "     except:\n",
    "          response = None\n",
    "     return response\n",
    " \n",
    "def get_soup(response):\n",
    "    if response is None:\n",
    "        return None\n",
    "    return BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "def get_login_time(url):\n",
    "    start_time = time.time()\n",
    "    response = get_request(url)\n",
    "    end_time = time.time()\n",
    "    load_time_in_seconds = end_time - start_time\n",
    "    return load_time_in_seconds, response\n",
    "\n",
    "def get_external_link(soup, url):\n",
    "    external_links = 0\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        if link.get(\"href\") and not url in link.get(\"href\"):\n",
    "            external_links += 1\n",
    "    return external_links\n",
    "\n",
    "def get_redirects(response):\n",
    "    num_redirects = len(response.history)\n",
    "    return num_redirects\n",
    "\n",
    "def get_ip_address_reputation(response):\n",
    "    try:\n",
    "        ip_addr = ipaddress.ip_address(response.url)\n",
    "        response_ip = requests.get(f\"http://checkip.dyndns.org/?ip={ip_addr}\")\n",
    "        reputation = \"safe\" if \"OK\" in response_ip.text else \"malicious\"\n",
    "    except:\n",
    "        reputation = None\n",
    "    return reputation\n",
    "\n",
    "def get_num_image(soup):\n",
    "    num_images = len(soup.find_all(\"img\"))\n",
    "    return num_images\n",
    "\n",
    "def get_num_iframes(soup):\n",
    "    num_iframes = len(soup.find_all(\"iframe\"))\n",
    "    return num_iframes\n",
    "\n",
    "def get_alexa_rank(soup):\n",
    "    try:\n",
    "        alexa_rank = None\n",
    "        for meta in soup.find_all(\"meta\"):\n",
    "            if \"name\" in meta.attrs and meta.attrs[\"name\"].lower() == \"alexa\":\n",
    "                alexa_rank = int(meta.attrs[\"content\"])\n",
    "    except:\n",
    "        alexa_rank = None\n",
    "    return alexa_rank\n",
    "\n",
    "def get_page_rank(url):\n",
    "    try:\n",
    "        GOOGLE_PR_CHECK_URL = \"http://toolbarqueries.google.com/tbr?client=navclient-auto&features=Rank&ch=%s&q=info:%s\"\n",
    "        domain = url.split(\"//\")[-1].split(\"/\")[0]\n",
    "        hsh = hash(domain.encode(\"utf-8\")) & 0xEFFFFFFF\n",
    "        response = requests.get(GOOGLE_PR_CHECK_URL % (hsh, domain))\n",
    "        if response.status_code == 200:\n",
    "            page_rank = int(response.content.strip().split(\":\")[-1])\n",
    "        else:\n",
    "            page_rank = None\n",
    "    except:\n",
    "        page_rank = None\n",
    "    return page_rank\n",
    "\n",
    "def get_num_hidden_text(soup):\n",
    "    try:\n",
    "        num_hidden_text = 0\n",
    "        for element in soup.find_all():\n",
    "            if element.get(\"style\") and \"display:none\" in element.get(\"style\").lower():\n",
    "                num_hidden_text += 1\n",
    "    except:\n",
    "        num_hidden_text = None\n",
    "    return num_hidden_text\n",
    "\n",
    "def get_ext_tot_ratio(soup, url):\n",
    "    try:\n",
    "        num_internal_links = 0\n",
    "        num_external_links = 0\n",
    "        for link in soup.find_all(\"a\"):\n",
    "            if link.get(\"href\"):\n",
    "                if url in link.get(\"href\"):\n",
    "                    num_internal_links += 1\n",
    "                else:\n",
    "                    num_external_links += 1\n",
    "        if num_internal_links > 0:\n",
    "            external_to_internal_ratio = num_external_links / num_internal_links\n",
    "        else:\n",
    "            external_to_internal_ratio = num_external_links\n",
    "    except:\n",
    "        external_to_internal_ratio = None\n",
    "    return external_to_internal_ratio\n",
    "\n",
    "def get_response_features(url):\n",
    "    time_and_response = get_login_time(url)\n",
    "    response = time_and_response[1]\n",
    "    if response is None:\n",
    "        return [0] * 7\n",
    "    soup = get_soup(response)\n",
    "    \n",
    "    # Features\n",
    "    time = time_and_response[0]\n",
    "    num_ex_links = get_external_link(soup, url)\n",
    "    num_redirects = get_redirects(response)\n",
    "    num_img = get_num_image(soup)\n",
    "    num_iframe = get_num_iframes(soup)\n",
    "    num_hidden = get_num_hidden_text(soup)\n",
    "    ext_tot_ratio = get_ext_tot_ratio(soup, url)\n",
    "    return [time, num_ex_links, num_redirects, num_img, num_iframe, num_hidden, ext_tot_ratio]\n",
    "\n",
    "def get_suspicious_words(url):\n",
    "    keywords = [\"login\", \"password\", \"verify\", \"account\", \"security\", \"wp\", \"admin\", \"content\",\n",
    "                \"site\", \"images\", \"js\", \"alibaba\", \"css\", \"myaccount\", \"dropbox\", \"themes\",\n",
    "                \"plugins\", \"signin\", \"view\"]\n",
    "    found_keywords = 0\n",
    "    for keyword in keywords:\n",
    "        if keyword in url:\n",
    "            found_keywords = found_keywords + 1\n",
    "    return found_keywords\n",
    "\n",
    "def has_ip_address(url):\n",
    "    ip_pattern = re.compile(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b')\n",
    "    return 1 if ip_pattern.search(url) else 0\n",
    "\n",
    "def is_url_shortened(url):\n",
    "    url_shortening_services = [\"bit.ly\", \"tinyurl.com\", \"goo.gl\", \"ow.ly\"]\n",
    "    return 1 if any(service in url for service in url_shortening_services) else 0\n",
    "\n",
    "\n",
    "def get_count_features(url):\n",
    "    length = len(url)\n",
    "    subdomain, domain = get_domain(url)\n",
    "    length = len(url)\n",
    "    subdomain_ratio = len(subdomain)/length\n",
    "\n",
    "    # f34 prefix suffix\n",
    "    hyphen_in_d = 1 if \"-\" in subdomain else 0\n",
    "\n",
    "    # f4-f24\n",
    "    num_dots = url.count('.')\n",
    "    num_www = url.count('www')\n",
    "    num_dcom = url.count('.com')\n",
    "    num_http = url.count('http')\n",
    "    num_https = url.count('https')\n",
    "    num_2slash = url.count('//')\n",
    "    num_quest = url.count('?')\n",
    "    num_prtc = url.count('%')\n",
    "    num_equal = url.count('=')\n",
    "    num_star = url.count('*')\n",
    "    num_dollar = url.count('$')\n",
    "    num_under = url.count('_')\n",
    "    num_space = url.count('%20') + url.count(' ')\n",
    "    num_slash = url.count('/')\n",
    "    num_dash = url.count('-')\n",
    "    num_at = url.count('@')\n",
    "    num_tile = url.count('~')\n",
    "    num_line = url.count('|')\n",
    "    num_colon = url.count(':')\n",
    "    num_semic = url.count(';')\n",
    "    num_comma = url.count(',')\n",
    "\n",
    "    return [length, subdomain_ratio, num_dots,\n",
    "            num_www, num_dcom, num_http, num_https, num_2slash, num_quest, num_prtc, num_equal,\n",
    "            num_star, num_dollar, num_under, num_space, num_slash, num_dash, num_at, num_tile,\n",
    "            num_line, num_colon, num_semic, num_comma]\n",
    "    \n",
    "def get_url_features(url):\n",
    "    return [get_suspicious_words(url)] + [has_ip_address(url)] + [is_url_shortened(url)] + get_count_features(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the models, imputer and scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "with open('/Users/yihongan/Desktop/tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Load the MinMaxScaler\n",
    "with open('/Users/yihongan/Desktop/std_sc.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# Load imputer\n",
    "with open('/Users/yihongan/Desktop/lr_imputer.pkl', 'rb') as f:\n",
    "    imputer = pickle.load(f)\n",
    "    \n",
    "# Load the CNN\n",
    "CNN = tf.keras.models.load_model('/Users/yihongan/Desktop/CNN.tf')\n",
    "\n",
    "# Load the DNN\n",
    "DNN = tf.keras.models.load_model('/Users/yihongan/Desktop/DNN.tf')\n",
    "\n",
    "# Load the rf\n",
    "with open('/Users/yihongan/Desktop/RF.pkl', 'rb') as f:\n",
    "    rf = pickle.load(f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for single url classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['age', 'num_suspicious_words', 'has_ip_address',\n",
    "                'is_url_shortened', 'length', 'subdomain_ratio', 'num_dots', 'num_www',\n",
    "                'num_dcom', 'num_http', 'num_https', 'num_2slash', 'num_quest',\n",
    "                'num_prtc', 'num_equal', 'num_star', 'num_dollar', 'num_under',\n",
    "                'num_space', 'num_slash', 'num_dash', 'num_at', 'num_tile', 'num_line',\n",
    "                'num_colon', 'num_semic', 'num_comma', 'login_time', 'num_ex_links',\n",
    "                'num_redirects', 'num_img', 'num_iframe', 'num_hidden', 'ext_tot_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_url(url, model=['CNN', 'DNN', 'RF']):\n",
    "    print(f'Test url: {url}')\n",
    "    # Numerical features\n",
    "    age = get_age(url)\n",
    "    response_features = get_response_features(url)\n",
    "    url_features = get_url_features(url)\n",
    "    \n",
    "    if response_features[0] == 0:\n",
    "        print(f'No response from {url}')\n",
    "    \n",
    "    X_num = [age] + url_features + response_features\n",
    " \n",
    "    if X_num[0] is None:\n",
    "        X_num = imputer.transform([X_num])\n",
    "        X_num_norm = scaler.transform(X_num)\n",
    "    else:\n",
    "        X_num_norm = scaler.transform(np.array([X_num]))\n",
    "    feature = pd.DataFrame(np.array(X_num).reshape(1, -1), columns=feature_names)\n",
    "    \n",
    "    # Textual features\n",
    "    text_input_shape = (100,)\n",
    "    X_text = np.array(pad_sequences(tokenizer.texts_to_sequences([url]), maxlen=text_input_shape[0], padding=\"post\"))\n",
    "\n",
    "    if model == 'CNN':\n",
    "        print('Model: CNN')\n",
    "        prob = CNN.predict([X_text, X_num_norm], verbose=0).squeeze()\n",
    "        pred = \"Phishing\" if prob > 0.5 else \"Legitimate\"\n",
    "    elif model == 'DNN':\n",
    "        print('Model: DNN')\n",
    "        prob = DNN.predict(X_num_norm).squeeze()\n",
    "        pred = \"Phishing\" if prob > 0.5 else \"Legitimate\"\n",
    "    else:\n",
    "        print('Model: Random Forest')\n",
    "        prob = rf.predict_proba(X_num_norm)[:, 1].squeeze()\n",
    "        pred = \"Phishing\" if prob > 0.5 else \"Legitimate\"\n",
    "\n",
    "    print(f\"Prediction: {pred}\")\n",
    "    print(f\"Predicted probability of being phishing: {prob}\")\n",
    "\n",
    "    return pred, feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with phishing website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test url: https://anazom.co.ip.azddeu.com/\n",
      "No response from https://anazom.co.ip.azddeu.com/\n",
      "Model: CNN\n",
      "Prediction: Phishing\n",
      "Predicted probability of being phishing: 0.9990572333335876\n"
     ]
    }
   ],
   "source": [
    "test_url = 'https://anazom.co.ip.azddeu.com/'\n",
    "pred, feature = predict_single_url(test_url, model='CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test url: https://www.aeombamk-co-jp.zanqianjia.com/\n",
      "No response from https://www.aeombamk-co-jp.zanqianjia.com/\n",
      "Model: Random Forest\n",
      "Prediction: Phishing\n",
      "Predicted probability of being phishing: 0.67\n"
     ]
    }
   ],
   "source": [
    "test_url = 'https://www.aeombamk-co-jp.zanqianjia.com/'\n",
    "pred, feature = predict_single_url(test_url, model='RF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test url: https://www.aeombamk-co-jp.zanqianjia.com/\n",
      "No response from https://www.aeombamk-co-jp.zanqianjia.com/\n",
      "Model: CNN\n",
      "Prediction: Phishing\n",
      "Predicted probability of being phishing: 0.9950657486915588\n"
     ]
    }
   ],
   "source": [
    "test_url = 'https://www.aeombamk-co-jp.zanqianjia.com/'\n",
    "pred, feature = predict_single_url(test_url, model='CNN')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legitimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test url: https://www.sta.cuhk.edu.hk/peoples/scpy/\n",
      "Model: CNN\n",
      "Prediction: Legitimate\n",
      "Predicted probability of being phishing: 0.004463085439056158\n"
     ]
    }
   ],
   "source": [
    "test_url = 'https://www.sta.cuhk.edu.hk/peoples/scpy/'\n",
    "pred, feature = predict_single_url(test_url, model='CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test url: https://www.google.com.hk\n",
      "Model: Random Forest\n",
      "Prediction: Legitimate\n",
      "Predicted probability of being phishing: 0.06\n"
     ]
    }
   ],
   "source": [
    "test_url = 'https://www.google.com.hk'\n",
    "pred, feature = predict_single_url(test_url, model='RF')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow-upgrade)",
   "language": "python",
   "name": "tensorflow-upgrade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
